{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Librerias necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krt08KDQm-y6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression,\n",
        "    Ridge,\n",
        "    Lasso,\n",
        "    ElasticNet,\n",
        "    RidgeCV,\n",
        "    ElasticNetCV,\n",
        "    LassoCV,\n",
        "    SGDRegressor,\n",
        "    LogisticRegression\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "     mean_squared_error, \n",
        "     r2_score, \n",
        "     mean_absolute_error,\n",
        "     classification_report, \n",
        "     confusion_matrix,\n",
        "     ConfusionMatrixDisplay,\n",
        "     balanced_accuracy_score, \n",
        "     log_loss,\n",
        "     roc_curve, \n",
        "     roc_auc_score, \n",
        "     auc,\n",
        "     accuracy_score\n",
        ")\n",
        "import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFQmvv5em-y7"
      },
      "outputs": [],
      "source": [
        "### Carga datos de dataset\n",
        "### Contiene aproximadamente 10 años de observaciones diarias de variables climáticas: temperatura, dirección y velocidad del viento, humedad, presión, nubosidad, y cantidad de lluvia en mm.\n",
        "### tras observar los datos del día de hoy, el objetivo es predecir las variables target:\n",
        "###                                                                                     -RainFallTomorrow: cantidad de lluvia del día posterior a la observación. Problema de Regresión.\n",
        "###                                                                                     -RainTomorrow: si el día siguiente llueve o no llueve. Problema de Clasificación.\n",
        "file_path = \"weatherAUS.csv\"\n",
        "df = pd.read_csv(file_path, sep=\",\", engine=\"python\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Limpieza y transformacion de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW6Sy0x0m-y9"
      },
      "source": [
        "Elimino la columna 'Unnamed: 0' porque es un indice que esta de mas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFQko9M7m-y9"
      },
      "outputs": [],
      "source": [
        "df = df.drop(\"Unnamed: 0\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoSfugqSm-y-"
      },
      "source": [
        "Segun el enunciado, unicamente nos interesan las ciudades Adelaide, Canberra, Cobar, Dartmoor, Melbourne, MelbourneAirport, MountGambier, Sydney y SydneyAirport por lo que filtro el DataSet para quedarme unicamente con los datos de dichas ciudades.\n",
        "\n",
        "Tambien elimino de una vez la variable 'Location' debido a que el enunicado declara que se pueden considerar como una unica ubicacion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGC8u8GPm-y-"
      },
      "outputs": [],
      "source": [
        "ciudades = [\n",
        "    \" Adelaide\",\n",
        "    \"Canberra\",\n",
        "    \"Cobar\",\n",
        "    \"Dartmoor\",\n",
        "    \"Melbourne\",\n",
        "    \"MelbourneAirport\",\n",
        "    \"MountGambier\",\n",
        "    \"Sydney\",\n",
        "    \"SydneyAirport\",\n",
        "]\n",
        "df = df[df[\"Location\"].isin(ciudades)]\n",
        "df = df.drop(\"Location\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTqfkGAOm-y-"
      },
      "source": [
        "Hago el split en df_train y df_test a partir de una fecha determinada para dejar aproximadamente un 80% de mis datos en Train y 20% en Test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7vR4HKGm-y_"
      },
      "outputs": [],
      "source": [
        "# Convierto la columna 'Date' a tipo datetime\n",
        "df[\"Date\"] = pd.to_datetime(df[\"Date\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fecha más antigua\n",
        "fecha_mas_antigua = df['Date'].min()\n",
        "fecha_mas_reciente = df['Date'].max()\n",
        "\n",
        "print(f'Fecha mas antigua: {fecha_mas_antigua}')\n",
        "print(f'Fecha mas reciente: {fecha_mas_reciente}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La fecha mas antigua del dataset es 01-11-2007 y la mas reciente es 24-06-2017. \n",
        "Decido hacer el split de datos a partir de la fecha 01-01-2016, concentrando aproximadamente el 80% de datos para el conjunto de entrenamiento, y el 20% restante para el conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsf_Umdlm-y_",
        "outputId": "c3a98ede-1187-43d4-a5fe-2c5da688ed30"
      },
      "outputs": [],
      "source": [
        "fecha_limite = \"2016-01-01\"\n",
        "\n",
        "df_train = df[df[\"Date\"] < fecha_limite]\n",
        "\n",
        "df_test = df[df[\"Date\"] >= fecha_limite]\n",
        "\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", len(df_train))\n",
        "print(\"Tamaño del conjunto de prueba:\", len(df_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUX1Ds6cm-y_"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tipos de datos y valores nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leHNCbYim-y_"
      },
      "source": [
        "Observo una descripcion, el tipo de dato y los valores nulos de cada variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "IWoeMij4m-zA",
        "outputId": "ff16c477-a1e0-457a-996a-122017dfedc9"
      },
      "outputs": [],
      "source": [
        "df_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7gKx0rZm-zA",
        "outputId": "0727a619-7d73-49d9-d4be-3088adaf242d"
      },
      "outputs": [],
      "source": [
        "df_train.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzoFl1g2m-zA",
        "outputId": "8f7d6a19-c0be-4ead-a5d4-dfa1242731d3"
      },
      "outputs": [],
      "source": [
        "df_train.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQto9G34m-zA"
      },
      "source": [
        "Observo que las variables 'RainToday', 'RainTomorrow' y 'RainfallTomorrow' tienen igual cantidad de valores nulos.\n",
        "\n",
        "Me fijo en que registros las tres columnas son nulas, son unicamente 570 registros, lo que representa aproximadamente un 2% de mi dataset, por lo que decido eliminarlos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "KA1IgPTcm-zA",
        "outputId": "fe6e1ef8-623c-4d12-aaf7-32b95a510a47"
      },
      "outputs": [],
      "source": [
        "# Registros de df_train donde las 3 variables son Nulas.\n",
        "df_train[\n",
        "    df_train[\"RainToday\"].isnull()\n",
        "    & df_train[\"RainTomorrow\"].isnull()\n",
        "    & df_train[\"RainfallTomorrow\"].isnull()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_k9e552m-zA"
      },
      "outputs": [],
      "source": [
        "df_train = df_train[\n",
        "    ~(\n",
        "        df_train[\"RainToday\"].isnull()\n",
        "        & df_train[\"RainTomorrow\"].isnull()\n",
        "        & df_train[\"RainfallTomorrow\"].isnull()\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgw8QmP5m-zA",
        "outputId": "d084a980-a076-449e-9181-07c723b6a854"
      },
      "outputs": [],
      "source": [
        "print(\"Nulos RainToday:\", df_train[\"RainToday\"].isnull().sum())\n",
        "print(\"Nulos RainTomorrow:\", df_train[\"RainTomorrow\"].isnull().sum())\n",
        "print(\"Nulos RainfallTomorrow:\", df_train[\"RainfallTomorrow\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMhvu6j_m-zB"
      },
      "source": [
        "En cada variable quedaron un total de 162 nulos, vuelvo a observar pero esta vez de a pares, en que variables hay nulos a la vez\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zumg1vn7m-zB",
        "outputId": "36392f86-3faf-4338-8f34-884be28a7e72"
      },
      "outputs": [],
      "source": [
        "# Observo de a pares df_train\n",
        "print(\n",
        "    \"Nulos RainToday y RainTomorrow:\",\n",
        "    (df_train[\"RainToday\"].isnull() & df_train[\"RainTomorrow\"].isnull()).sum(),\n",
        ")\n",
        "print(\n",
        "    \"Nulos RainToday y RainfallTomorrow:\",\n",
        "    (df_train[\"RainToday\"].isnull() & df_train[\"RainfallTomorrow\"].isnull()).sum(),\n",
        ")\n",
        "print(\n",
        "    \"Nulos RainTomorrow y RainfallTomorrow:\",\n",
        "    (df_train[\"RainTomorrow\"].isnull() & df_train[\"RainfallTomorrow\"].isnull()).sum(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLMdito9m-zB"
      },
      "source": [
        "Procedo a eliminar los registros nulos de las variables 'RainTomorrow' y 'RainfallTomorrow'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "A9wLtIImm-zB",
        "outputId": "e6df8039-3cc8-4672-c7c3-db12a9ef2ab8"
      },
      "outputs": [],
      "source": [
        "df_train[df_train[\"RainTomorrow\"].isnull() & df_train[\"RainfallTomorrow\"].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2Rz4ssSm-zB"
      },
      "outputs": [],
      "source": [
        "df_train = df_train[\n",
        "    ~(df_train[\"RainTomorrow\"].isnull() & df_train[\"RainfallTomorrow\"].isnull())\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Matriz de correlacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gy2QJUgxm-zB",
        "outputId": "4d762801-0742-4a23-f22d-8a1d56065330"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(\n",
        "    df_train[\n",
        "        [\n",
        "            \"MinTemp\",\n",
        "            \"MaxTemp\",\n",
        "            \"Rainfall\",\n",
        "            \"Evaporation\",\n",
        "            \"Sunshine\",\n",
        "            \"WindGustSpeed\",\n",
        "            \"WindSpeed9am\",\n",
        "            \"WindSpeed3pm\",\n",
        "            \"Humidity9am\",\n",
        "            \"Humidity3pm\",\n",
        "            \"Pressure9am\",\n",
        "            \"Pressure3pm\",\n",
        "            \"Cloud9am\",\n",
        "            \"Cloud3pm\",\n",
        "            \"Temp9am\",\n",
        "            \"Temp3pm\",\n",
        "            \"RainfallTomorrow\",\n",
        "        ]\n",
        "    ].corr(),\n",
        "    annot=True,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivq8i95Xm-zB"
      },
      "source": [
        "Para rellenar valores nulos, decidi agregar una columna a mi df donde especifico el **Bimestre del año** al que pertenece cada registro, esto lo hago para tener de alguna manera los datos mas segmentados y no calcular una Media, Mediana, o lo que corresponda sobre todos los datos juntos ya que por ejemplo, las temperaturas, vientos, lluvias, etc. pueden no ser lo mismo al inicio del año como por la mitad o al final.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiEtqfJgm-zB"
      },
      "outputs": [],
      "source": [
        "def determinar_bimestre(fecha):\n",
        "    mes = fecha.month\n",
        "    if 1 <= mes <= 2:\n",
        "        return \"Bimestre 1\"\n",
        "    elif 3 <= mes <= 4:\n",
        "        return \"Bimestre 2\"\n",
        "    elif 5 <= mes <= 6:\n",
        "        return \"Bimestre 3\"\n",
        "    elif 7 <= mes <= 8:\n",
        "        return \"Bimestre 4\"\n",
        "    elif 9 <= mes <= 10:\n",
        "        return \"Bimestre 5\"\n",
        "    else:\n",
        "        return \"Bimestre 6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN64FZfRm-zB",
        "outputId": "b67ac864-a380-45e2-9283-8d1a70c95e54"
      },
      "outputs": [],
      "source": [
        "df_train[\"Bimestre\"] = df_train[\"Date\"].apply(lambda x: determinar_bimestre(x))\n",
        "\n",
        "df_test[\"Bimestre\"] = df_test[\"Date\"].apply(lambda x: determinar_bimestre(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxzQGMaEm-zB"
      },
      "source": [
        "### Variable: Rainfall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfLosGjem-zB",
        "outputId": "2664dfa2-8b3a-4312-e044-92d6ba7ee1aa"
      },
      "outputs": [],
      "source": [
        "print(df_train[\"Rainfall\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "FSDUvLklm-zC",
        "outputId": "9378f709-443b-4fd0-ae81-535bb99cc585"
      },
      "outputs": [],
      "source": [
        "# Diagrama de densidades de la variable RainFall\n",
        "bimestre_1 = df_train[df_train[\"Bimestre\"] == \"Bimestre 1\"]\n",
        "bimestre_2 = df_train[df_train[\"Bimestre\"] == \"Bimestre 2\"]\n",
        "bimestre_3 = df_train[df_train[\"Bimestre\"] == \"Bimestre 3\"]\n",
        "bimestre_4 = df_train[df_train[\"Bimestre\"] == \"Bimestre 4\"]\n",
        "bimestre_5 = df_train[df_train[\"Bimestre\"] == \"Bimestre 5\"]\n",
        "bimestre_6 = df_train[df_train[\"Bimestre\"] == \"Bimestre 6\"]\n",
        "\n",
        "\n",
        "bandwidth = 0.5\n",
        "sns.kdeplot(\n",
        "    data=bimestre_1[\"Rainfall\"], fill=True, label=\"bimestre_1\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_2[\"Rainfall\"], fill=True, label=\"bimestre_2\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_3[\"Rainfall\"], fill=True, label=\"bimestre_3\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_4[\"Rainfall\"], fill=True, label=\"bimestre_4\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_5[\"Rainfall\"], fill=True, label=\"bimestre_5\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_6[\"Rainfall\"], fill=True, label=\"bimestre_6\", bw_adjust=bandwidth\n",
        ")\n",
        "plt.title(\"Diagrama de Densidad de lluvias por Bimestre\")\n",
        "plt.xlabel(\"Rainfall\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_df7fpKm-zC"
      },
      "source": [
        "Puedo apreciar que la densidad de lluvia no depende del bimestre, por lo que, en este caso, no haria falta hacer esta diferenciacion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "ekeM2C1Um-zC",
        "outputId": "3499125f-13be-4c63-ecea-26751bcaee68"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=\"Rainfall\", data=df_train)\n",
        "plt.title(\"Boxplot de Rainfall\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EcZeLuBm-zC"
      },
      "source": [
        "Veo una gran presencia de valores Outliers por lo que me inclino a usar la Mediana como medida para rellenar los valores nulos de la variable RainFall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1XhTBq9m-zC",
        "outputId": "15459b74-c166-4e27-f8da-bc5612bbf14b"
      },
      "outputs": [],
      "source": [
        "mediana_por_dia_train = df_train.groupby(df[\"Date\"].dt.date)[\"Rainfall\"].median()\n",
        "\n",
        "df_train[\"Rainfall\"] = df_train.apply(\n",
        "    lambda row: (\n",
        "        mediana_por_dia_train[row[\"Date\"].date()]\n",
        "        if pd.isnull(row[\"Rainfall\"])\n",
        "        else row[\"Rainfall\"]\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "mediana_por_dia_test = df_test.groupby(df[\"Date\"].dt.date)[\"Rainfall\"].median()\n",
        "\n",
        "df_test[\"Rainfall\"] = df_test.apply(\n",
        "    lambda row: (\n",
        "        mediana_por_dia_test[row[\"Date\"].date()]\n",
        "        if pd.isnull(row[\"Rainfall\"])\n",
        "        else row[\"Rainfall\"]\n",
        "    ),\n",
        "    axis=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoTJtnuom-zD"
      },
      "source": [
        "##### Variable: Evaporation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBW_fUcYm-zD",
        "outputId": "50898f72-9199-4506-e97a-6fcdcbcbc55e"
      },
      "outputs": [],
      "source": [
        "print(df_train[\"Evaporation\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "CwOKhQUxm-zD",
        "outputId": "55a77dac-45e1-423c-ad35-10c43d39f7b4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=\"Evaporation\", data=df_train, color=\"green\")\n",
        "plt.title(\"Boxplot de Evaporation\")\n",
        "# plt.ylabel(\"MaxTemp\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Y6I8yrm-zD"
      },
      "source": [
        "Veo una gran presencia de valores Outliers por lo que me inclino a usar la Mediana como medida para rellenar los valores nulos de la variable **Evaporation**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "LOa4aECym-zD",
        "outputId": "f6c82eef-962c-419d-fe28-bdb5f8f54944"
      },
      "outputs": [],
      "source": [
        "# Diagrama de densidades de la variable Evaporation\n",
        "bimestre_1 = df_train[df_train[\"Bimestre\"] == \"Bimestre 1\"]\n",
        "bimestre_2 = df_train[df_train[\"Bimestre\"] == \"Bimestre 2\"]\n",
        "bimestre_3 = df_train[df_train[\"Bimestre\"] == \"Bimestre 3\"]\n",
        "bimestre_4 = df_train[df_train[\"Bimestre\"] == \"Bimestre 4\"]\n",
        "bimestre_5 = df_train[df_train[\"Bimestre\"] == \"Bimestre 5\"]\n",
        "bimestre_6 = df_train[df_train[\"Bimestre\"] == \"Bimestre 6\"]\n",
        "\n",
        "\n",
        "bandwidth = 0.5\n",
        "sns.kdeplot(\n",
        "    data=bimestre_1[\"Evaporation\"], fill=True, label=\"bimestre_1\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_2[\"Evaporation\"], fill=True, label=\"bimestre_2\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_3[\"Evaporation\"], fill=True, label=\"bimestre_3\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_4[\"Evaporation\"], fill=True, label=\"bimestre_4\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_5[\"Evaporation\"], fill=True, label=\"bimestre_5\", bw_adjust=bandwidth\n",
        ")\n",
        "sns.kdeplot(\n",
        "    data=bimestre_6[\"Evaporation\"], fill=True, label=\"bimestre_6\", bw_adjust=bandwidth\n",
        ")\n",
        "plt.title(\"Diagrama de Densidad de Evaporation por Bimestre\")\n",
        "plt.xlabel(\"Evaporation\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iCw5PFrm-zD"
      },
      "source": [
        "En este caso se puede apreciar una variacion en la densidad de la variable Evaporation respecto del bimestre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7IAV_MIm-zD"
      },
      "outputs": [],
      "source": [
        "bim = df_train.groupby(\"Bimestre\")\n",
        "medians = bim[\"Evaporation\"].median()\n",
        "\n",
        "for bimestre, median in medians.items():\n",
        "    df_train.loc[\n",
        "        (df_train[\"Bimestre\"] == bimestre) & (df_train[\"Evaporation\"].isnull()),\n",
        "        \"Evaporation\",\n",
        "    ] = median\n",
        "\n",
        "\n",
        "for bimestre, median in medians.items():\n",
        "    df_test.loc[\n",
        "        (df_test[\"Bimestre\"] == bimestre) & (df_test[\"Evaporation\"].isnull()),\n",
        "        \"Evaporation\",\n",
        "    ] = median"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk0kk_gCm-zD"
      },
      "source": [
        "##### Variable: Sunshine\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfxLu1XSm-zE",
        "outputId": "aa0a7c9c-aaf8-4fba-c4d8-a6e45adaf4c5"
      },
      "outputs": [],
      "source": [
        "print(df_train[\"Sunshine\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "0E5a57nJm-zE",
        "outputId": "efe528ff-4b16-44a9-8d31-55b887702d24"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(x=\"Sunshine\", data=df_train, color=\"orange\")\n",
        "plt.title(\"Boxplot de Sunshine\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ANKuAqgm-zE"
      },
      "source": [
        "La distribucion de la variable Sunshine se ve bastante balanceada y sin presencia de outliers por lo que utilizo la **Media** para imputar a los valores nulos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYEi2m9Tm-zE",
        "outputId": "5d771331-e245-4f9b-a2fa-559e41bb1395"
      },
      "outputs": [],
      "source": [
        "df_train[\"Sunshine\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\"Sunshine\"].transform(\n",
        "    lambda x: x.fillna(x.mean())\n",
        ")\n",
        "\n",
        "df_test[\"Sunshine\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\"Sunshine\"].transform(\n",
        "    lambda x: x.fillna(x.mean())\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEg0q0h8m-zE"
      },
      "source": [
        "##### Variables: WindGustDir, WindDir9am y WindDir3pm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVoRYh96m-zE",
        "outputId": "aa1107df-0678-4bb2-dec0-6378197bddeb"
      },
      "outputs": [],
      "source": [
        "print(df_train[\"WindGustDir\"].isnull().sum())\n",
        "print(df_train[\"WindDir9am\"].isnull().sum())\n",
        "print(df_train[\"WindDir3pm\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKRX9imjm-zE"
      },
      "source": [
        "Relleno los valores faltantes para cada variable utilizando la **Moda** de cada dia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayBSLdaYm-zE",
        "outputId": "2001c069-7607-423d-d160-6dc5668f17ad"
      },
      "outputs": [],
      "source": [
        "df_train[\"WindGustDir\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"WindGustDir\"\n",
        "].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "df_train[\"WindDir9am\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"WindDir9am\"\n",
        "].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "df_train[\"WindDir3pm\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"WindDir3pm\"\n",
        "].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "\n",
        "\n",
        "df_test[\"WindGustDir\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"WindGustDir\"\n",
        "].transform(lambda x: x.fillna(x.mode().iloc[0]))\n",
        "df_test[\"WindDir9am\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\"WindDir9am\"].transform(\n",
        "    lambda x: x.fillna(x.mode().iloc[0])\n",
        ")\n",
        "df_test[\"WindDir3pm\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\"WindDir3pm\"].transform(\n",
        "    lambda x: x.fillna(x.mode().iloc[0])\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHGyrK2Lm-zE"
      },
      "source": [
        "##### Variables: WindGustSpeed, WindSpeed9am y WindSpeed3pm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "xG9nr2Xam-zF",
        "outputId": "446656b7-1481-4b6f-eb17-052ab52a1d61"
      },
      "outputs": [],
      "source": [
        "data_to_plot = df_train[[\"WindGustSpeed\", \"WindSpeed9am\", \"WindSpeed3pm\"]]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data_to_plot, palette=\"Set3\")\n",
        "plt.title(\"Boxplots de WindSpeed9am, WindSpeed3pm y WindGustSpeed\")\n",
        "plt.ylabel(\"Speed\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15f3zDCDm-zF"
      },
      "source": [
        "Se observa una gran presencia de outlaiers en las 3 variables por lo que procedo a imputar los valores nulos utilizando la **Mediana**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u35wOb6Dm-zF",
        "outputId": "c7904d6c-8d64-4dbf-9870-c00a8ee462b4"
      },
      "outputs": [],
      "source": [
        "df_train[\"WindGustSpeed\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"WindGustSpeed\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "df_train[\"WindSpeed9am\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"WindSpeed9am\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "df_train[\"WindSpeed3pm\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"WindGustSpeed\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "\n",
        "df_test[\"WindGustSpeed\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"WindGustSpeed\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "df_test[\"WindSpeed9am\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"WindSpeed9am\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "df_test[\"WindSpeed3pm\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"WindGustSpeed\"\n",
        "].transform(lambda x: x.fillna(x.median()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7DzK91Km-zF"
      },
      "source": [
        "Genero una nueva columna llamada 'Dif_WindSpeed' imputandole el valor correspondiente a la diferencia de las columnas 'WindSpeed9am' y 'WindSpeed3pm' **( 'WindSpeed9am' - 'WindSpeed3pm' )**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4tYNkp_m-zF",
        "outputId": "e9afdd23-7f60-4dd2-b248-718690287f1b"
      },
      "outputs": [],
      "source": [
        "df_train[\"WindSpeed_Difference\"] = df_train[\"WindSpeed9am\"] - df_train[\"WindSpeed3pm\"]\n",
        "df_train.drop([\"WindSpeed9am\", \"WindSpeed3pm\"], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "df_test[\"WindSpeed_Difference\"] = df_test[\"WindSpeed9am\"] - df_test[\"WindSpeed3pm\"]\n",
        "df_test.drop([\"WindSpeed9am\", \"WindSpeed3pm\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T90gCplwm-zF"
      },
      "source": [
        "##### Variables: Humidity9am, Humidity3pm, Cloud9am, Cloud3pm, Pressure9am y Pressure3pm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNSS0Oc8m-zF",
        "outputId": "1227b66b-4104-472e-c4b7-918944fe2660"
      },
      "outputs": [],
      "source": [
        "print(df_train[\"Humidity9am\"].isnull().sum())\n",
        "print(df_train[\"Humidity3pm\"].isnull().sum())\n",
        "print(df_train[\"Cloud9am\"].isnull().sum())\n",
        "print(df_train[\"Cloud3pm\"].isnull().sum())\n",
        "print(df_train[\"Pressure9am\"].isnull().sum())\n",
        "print(df_train[\"Pressure3pm\"].isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "d1m8fwHym-zF",
        "outputId": "aeb8dbb6-069b-4f0c-ac6a-4e63aed7203e"
      },
      "outputs": [],
      "source": [
        "data_to_plot = df_train[[\"Humidity9am\", \"Humidity3pm\"]]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data_to_plot)\n",
        "plt.title(\"Boxplots de Humidity9am y Humidity3pm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PMMUkRXm-zF",
        "outputId": "be3207c4-7417-4069-ec95-8e578803194f"
      },
      "outputs": [],
      "source": [
        "df_train[\"Humidity9am\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"Humidity9am\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "df_train[\"Humidity3pm\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"Humidity3pm\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_test[\"Humidity9am\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"Humidity9am\"\n",
        "].transform(lambda x: x.fillna(x.median()))\n",
        "df_test[\"Humidity3pm\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"Humidity3pm\"\n",
        "].transform(lambda x: x.fillna(x.median()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwRbt_A4m-zF"
      },
      "source": [
        "Genero una nueva columna llamada 'Dif_Humidity' imputandole el valor correspondiente a la diferencia de las columnas 'Humidity9am' y 'HUmidity3pm' **( 'Humidity9am' - 'Humidity3pm' )**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fuf93_F_m-zG",
        "outputId": "3d19441d-3e62-4266-8f52-f921d5c9080b"
      },
      "outputs": [],
      "source": [
        "df_train[\"Humidity_Difference\"] = df_train[\"Humidity9am\"] - df_train[\"Humidity3pm\"]\n",
        "df_train.drop([\"Humidity9am\", \"Humidity3pm\"], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "df_test[\"Humidity_Difference\"] = df_test[\"Humidity9am\"] - df_test[\"Humidity3pm\"]\n",
        "df_test.drop([\"Humidity9am\", \"Humidity3pm\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "-nevjuaAm-zG",
        "outputId": "38f086f5-0286-463d-dd43-7090c2a131b9"
      },
      "outputs": [],
      "source": [
        "plt.hist(df_train[\"Cloud9am\"], bins=20, color=\"skyblue\", alpha=0.7, label=\"Cloud9am\")\n",
        "plt.hist(df_train[\"Cloud3pm\"], bins=20, color=\"salmon\", alpha=0.7, label=\"Cloud3pm\")\n",
        "plt.xlabel(\"Nubosidad\")\n",
        "plt.ylabel(\"Frecuencia\")\n",
        "plt.title(\"Histograma de Cloud9am y Cloud3pm\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vpCIkcpm-zG",
        "outputId": "22c6ab25-6811-430e-a8f2-e62af6ae275e"
      },
      "outputs": [],
      "source": [
        "df_train[\"Cloud9am\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\"Cloud9am\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "df_train[\"Cloud3pm\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\"Cloud3pm\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "\n",
        "df_test[\"Cloud9am\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\"Cloud9am\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "df_test[\"Cloud3pm\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\"Cloud3pm\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XFSoH2fm-zG"
      },
      "source": [
        "Genero una nueva columna llamada 'Dif_Cloud' imputandole el valor correspondiente a la diferencia de las columnas 'Humidity9am' y 'HUmidity3pm' **( 'Cloud9am' - 'Cloud3pm' )**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzsA9oSfm-zG",
        "outputId": "d283eab6-bb43-4064-bbb8-c6fcdecbf41b"
      },
      "outputs": [],
      "source": [
        "df_train[\"Cloud_Difference\"] = df_train[\"Cloud9am\"] - df_train[\"Cloud3pm\"]\n",
        "df_train.drop([\"Cloud9am\", \"Cloud3pm\"], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "df_test[\"Cloud_Difference\"] = df_test[\"Cloud9am\"] - df_test[\"Cloud3pm\"]\n",
        "df_test.drop([\"Cloud9am\", \"Cloud3pm\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Oq5j4U4gm-zG",
        "outputId": "96691c76-1257-42fd-d292-90cf833e4d6e"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(df_train[\"Pressure9am\"], color=\"skyblue\", label=\"Pressure9am\", fill=True)\n",
        "sns.kdeplot(df_train[\"Pressure3pm\"], color=\"salmon\", label=\"Pressure3pm\", fill=True)\n",
        "plt.xlabel(\"Pressure\")\n",
        "plt.ylabel(\"Densidad\")\n",
        "plt.title(\"Gráfico de Densidad de Pressure9am y Pressure3pm\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWPzC3uum-zG"
      },
      "source": [
        "Observando la distribucion de las variables 'Pressure9am' y 'Pressure3pm' se observa una distribucion normal, por lo que decido imputar los valores nulos utilizando la **Media**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAtk6Vqhm-zG",
        "outputId": "c988bce7-f133-4dec-9a95-4f637d03ed8a"
      },
      "outputs": [],
      "source": [
        "df_train[\"Pressure9am\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"Pressure9am\"\n",
        "].transform(lambda x: x.fillna(x.mean()))\n",
        "df_train[\"Pressure3pm\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\n",
        "    \"Pressure3pm\"\n",
        "].transform(lambda x: x.fillna(x.mean()))\n",
        "\n",
        "\n",
        "df_test[\"Pressure9am\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"Pressure9am\"\n",
        "].transform(lambda x: x.fillna(x.mean()))\n",
        "df_test[\"Pressure3pm\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\n",
        "    \"Pressure3pm\"\n",
        "].transform(lambda x: x.fillna(x.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQe4ErR_m-zG"
      },
      "source": [
        "Genero una nueva columna llamada 'Dif_Pressure' imputandole el valor correspondiente a la diferencia de las columnas 'Pressure9am' y 'Pressure3pm' **( 'Pressure9am' - 'Pressure3pm' )**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YL3GVN5m-zH",
        "outputId": "d0005e30-2f89-441b-e971-68faececde24"
      },
      "outputs": [],
      "source": [
        "df_train[\"Pressure_Difference\"] = df_train[\"Pressure9am\"] - df_train[\"Pressure3pm\"]\n",
        "df_train.drop([\"Pressure9am\", \"Pressure3pm\"], axis=1, inplace=True)\n",
        "\n",
        "\n",
        "df_test[\"Pressure_Difference\"] = df_test[\"Pressure9am\"] - df_test[\"Pressure3pm\"]\n",
        "df_test.drop([\"Pressure9am\", \"Pressure3pm\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgJe3Kydm-zH"
      },
      "source": [
        "##### Variables: MaxTemp, MinTemp, Temp9am y Temp3pm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "ZHwkpjLTm-zH",
        "outputId": "e5bcc3bd-fc72-43fb-d269-a34aa0a5f03f"
      },
      "outputs": [],
      "source": [
        "# Calcula la media de temperatura mínima para cada bimestre\n",
        "mean_temps = df_train.groupby(\"Bimestre\")[\"MinTemp\"].mean().reset_index()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x=\"Bimestre\",\n",
        "    y=\"MinTemp\",\n",
        "    data=mean_temps,\n",
        "    hue=\"Bimestre\",\n",
        "    palette=\"Set1\",\n",
        "    dodge=False,\n",
        "    legend=False,\n",
        ")\n",
        "plt.title(\"Media de Temperatura Mínima por Bimestre\")\n",
        "plt.xlabel(\"Bimestre\")\n",
        "plt.ylabel(\"Temperatura Mínima Media (°C)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "8y4cqVWom-zH",
        "outputId": "08b8fbca-886a-4d70-d620-ce1f28d4a6b6"
      },
      "outputs": [],
      "source": [
        "# Calcula la media de temperatura mínima para cada bimestre\n",
        "mean_temps = df_train.groupby(\"Bimestre\")[\"MaxTemp\"].mean().reset_index()\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x=\"Bimestre\",\n",
        "    y=\"MaxTemp\",\n",
        "    data=mean_temps,\n",
        "    hue=\"Bimestre\",\n",
        "    palette=\"Set2\",\n",
        "    dodge=False,\n",
        "    legend=False,\n",
        ")\n",
        "plt.title(\"Media de Temperatura Maxima por Bimestre\")\n",
        "plt.xlabel(\"Bimestre\")\n",
        "plt.ylabel(\"Temperatura Mínima Media (°C)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "5-L0ydQlm-zH",
        "outputId": "eab4f7d0-d6be-4723-8f83-8369a6c94e96"
      },
      "outputs": [],
      "source": [
        "data_to_plot = df_train[[\"MinTemp\", \"MaxTemp\"]]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data_to_plot, palette=\"Set1\")\n",
        "plt.title(\"Boxplots de MinTemp y MaxTemp\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvi-Qfplm-zH"
      },
      "source": [
        "Puedo apreciar variaciones en los valores tanto en la variable MinTemp como MaxTemp dependiendo del bimestre del año, a la vez observo outliers por lo que en este caso decido rellenar los valores nulos con la Mediana del bimestre correspondiente a cada registro.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mknlVLXm-zH"
      },
      "outputs": [],
      "source": [
        "median_min_temp_by_bimestre_train = df_train.groupby(\"Bimestre\")[\"MinTemp\"].median()\n",
        "\n",
        "for bimestre, median_temp in median_min_temp_by_bimestre_train.items():\n",
        "    df_train.loc[df_train[\"Bimestre\"] == bimestre, \"MinTemp\"] = df_train.loc[\n",
        "        df_train[\"Bimestre\"] == bimestre, \"MinTemp\"\n",
        "    ].fillna(median_temp)\n",
        "\n",
        "for bimestre, median_temp in median_min_temp_by_bimestre_train.items():\n",
        "    df_test.loc[df_test[\"Bimestre\"] == bimestre, \"MinTemp\"] = df_test.loc[\n",
        "        df_test[\"Bimestre\"] == bimestre, \"MinTemp\"\n",
        "    ].fillna(median_temp)\n",
        "\n",
        "\n",
        "median_max_temp_by_bimestre_train = df_train.groupby(\"Bimestre\")[\"MaxTemp\"].median()\n",
        "\n",
        "for bimestre, median_temp in median_max_temp_by_bimestre_train.items():\n",
        "    df_train.loc[df_train[\"Bimestre\"] == bimestre, \"MaxTemp\"] = df_train.loc[\n",
        "        df_train[\"Bimestre\"] == bimestre, \"MaxTemp\"\n",
        "    ].fillna(median_temp)\n",
        "\n",
        "for bimestre, median_temp in median_max_temp_by_bimestre_train.items():\n",
        "    df_test.loc[df_test[\"Bimestre\"] == bimestre, \"MaxTemp\"] = df_test.loc[\n",
        "        df_test[\"Bimestre\"] == bimestre, \"MaxTemp\"\n",
        "    ].fillna(median_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "GHkGj8FTm-zH",
        "outputId": "2fd66efc-53c6-4465-b143-9d0fb3e0942f"
      },
      "outputs": [],
      "source": [
        "data_to_plot = df_train[[\"Temp9am\", \"Temp3pm\"]]\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=data_to_plot, palette=\"Set2\")\n",
        "plt.title(\"Boxplots de Temp9am y Temp3pm\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8QXIpBkm-zH"
      },
      "source": [
        "Completo los valores nulos de Temp9am y Temp3pm con la mediana por dia debido a la presencia de outliers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzBVVLNmm-zI",
        "outputId": "ce63e1a4-3493-4fed-ce3c-5550ac0a06b4"
      },
      "outputs": [],
      "source": [
        "df_train[\"Temp9am\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\"Temp9am\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "df_train[\"Temp3pm\"] = df_train.groupby(df_train[\"Date\"].dt.day)[\"Temp3pm\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "\n",
        "df_test[\"Temp9am\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\"Temp9am\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")\n",
        "df_test[\"Temp3pm\"] = df_test.groupby(df_test[\"Date\"].dt.day)[\"Temp3pm\"].transform(\n",
        "    lambda x: x.fillna(x.median())\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRy0bQt-m-zI",
        "outputId": "668357b0-990a-4899-f87a-c81dbc399f5b"
      },
      "outputs": [],
      "source": [
        "df_train.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpmQD40lm-zI"
      },
      "source": [
        "Genero una nueva columna llamada 'Dif_Temp' imputandole el valor correspondiente a la diferencia de las columnas 'Temp3pm' y 'Temp9am' **( 'Temp3pm' - 'Temp9am' )**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3hiOGbRm-zI",
        "outputId": "e6f555b2-d5b8-4bf3-bb41-b0f7e33ab6fe"
      },
      "outputs": [],
      "source": [
        "df_train[\"Temp_Difference\"] = df_train[\"Temp3pm\"] - df_train[\"Temp9am\"]\n",
        "df_train.drop([\"Temp3pm\", \"Temp9am\"], axis=1, inplace=True)\n",
        "\n",
        "df_test[\"Temp_Difference\"] = df_test[\"Temp3pm\"] - df_test[\"Temp9am\"]\n",
        "df_test.drop([\"Temp3pm\", \"Temp9am\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKwtr38ym-zI"
      },
      "source": [
        "Genero una nueva columna llamada 'Dif_Temp_Max_Min' imputandole el valor correspondiente a la diferencia de las columnas 'MaxTemp' y 'MinTemp' **( 'MaxTemp' - 'MinTemp' )**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_UuefyCm-zI",
        "outputId": "960f08c6-dff1-46ca-cc87-ae6849aecec4"
      },
      "outputs": [],
      "source": [
        "df_train[\"Dif_Temp_Max_Min\"] = df_train[\"MaxTemp\"] - df_train[\"MinTemp\"]\n",
        "df_train.drop([\"MaxTemp\", \"MinTemp\"], axis=1, inplace=True)\n",
        "\n",
        "df_test[\"Dif_Temp_Max_Min\"] = df_test[\"MaxTemp\"] - df_test[\"MinTemp\"]\n",
        "df_test.drop([\"MaxTemp\", \"MinTemp\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHxGTAvsm-zI"
      },
      "source": [
        "##### Variables: RainToday y RainTomorrow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KEgW7W4m-zI"
      },
      "source": [
        "Relleno los valores nulos de 'RainToday' con la **Moda** por dia.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hyHo8Jum-zI",
        "outputId": "e6999885-e9ab-4de1-ea5c-952fdfcde71d"
      },
      "outputs": [],
      "source": [
        "print(df_train[\"RainToday\"].isna().sum())\n",
        "print(df_train[\"RainTomorrow\"].isna().sum())\n",
        "print(df_train[\"RainfallTomorrow\"].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz-aDOc3m-zI",
        "outputId": "beb03969-f93f-437e-e88f-44700287238c"
      },
      "outputs": [],
      "source": [
        "moda_RainToday_train = df_train.groupby(\"Date\")[\"RainToday\"].transform(\n",
        "    lambda x: x.mode().iloc[0] if not x.mode().empty else None\n",
        ")\n",
        "df_train[\"RainToday\"] = df_train[\"RainToday\"].fillna(moda_RainToday_train)\n",
        "\n",
        "moda_RainToday_test = df_test.groupby(\"Date\")[\"RainToday\"].transform(\n",
        "    lambda x: x.mode().iloc[0] if not x.mode().empty else None\n",
        ")\n",
        "df_test[\"RainToday\"] = df_test[\"RainToday\"].fillna(moda_RainToday_test)\n",
        "\n",
        "\n",
        "moda_RainTomorrow_test = df_test.groupby(\"Date\")[\"RainTomorrow\"].transform(\n",
        "    lambda x: x.mode().iloc[0] if not x.mode().empty else None\n",
        ")\n",
        "df_test[\"RainTomorrow\"] = df_test[\"RainTomorrow\"].fillna(moda_RainTomorrow_test)\n",
        "\n",
        "moda_RainfallTomorrow_test = df_test.groupby(\"Date\")[\"RainfallTomorrow\"].transform(\n",
        "    lambda x: x.mode().iloc[0] if not x.mode().empty else None\n",
        ")\n",
        "df_test[\"RainfallTomorrow\"] = df_test[\"RainfallTomorrow\"].fillna(\n",
        "    moda_RainfallTomorrow_test\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4GTFNHmm-zJ",
        "outputId": "79fbd6ca-2d4f-4fb0-edaa-eea79b7c9d7b"
      },
      "outputs": [],
      "source": [
        "df_test.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iN6B_eHsm-zJ",
        "outputId": "4f374d27-38a8-45cc-f5ea-e279ea87271e"
      },
      "outputs": [],
      "source": [
        "print(df_train[\"RainToday\"].isna().sum())\n",
        "print(df_train[\"RainTomorrow\"].isna().sum())\n",
        "print(df_train[\"RainfallTomorrow\"].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUbWrdzMm-zJ"
      },
      "source": [
        "La columna 'RainToday' y 'RainTomorrow' tienen valores 'Yes' 'No' por lo que los mapeo a 1 para 'Yes' y 0 para 'No'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8lZztURm-zJ",
        "outputId": "362413f3-6d36-473c-99c1-65e44307162c"
      },
      "outputs": [],
      "source": [
        "# df_train\n",
        "df_train[\"RainToday\"] = df_train[\"RainToday\"].map({\"Yes\": 1, \"No\": 0})\n",
        "df_train[\"RainTomorrow\"] = df_train[\"RainTomorrow\"].map({\"Yes\": 1, \"No\": 0})\n",
        "# df_test\n",
        "df_test[\"RainToday\"] = df_test[\"RainToday\"].map({\"Yes\": 1, \"No\": 0})\n",
        "df_test[\"RainTomorrow\"] = df_test[\"RainTomorrow\"].map({\"Yes\": 1, \"No\": 0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p-GqX2Hm-zJ",
        "outputId": "dc41027a-cdc8-45c6-f5f8-abed5599e881"
      },
      "outputs": [],
      "source": [
        "df_train.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D83imC1Gm-zJ"
      },
      "source": [
        "### Dummies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP_wSNAKm-zJ",
        "outputId": "82c9755b-2f91-44d1-c64e-9171ed7c1ab3"
      },
      "outputs": [],
      "source": [
        "df_train[\"WindGustDir\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Go5Ger4m-zJ",
        "outputId": "1f993b3c-d5d8-43bf-d939-c0f88d74a11a"
      },
      "outputs": [],
      "source": [
        "orien = [\n",
        "    \"SSW\",\n",
        "    \"S\",\n",
        "    \"SE\",\n",
        "    \"NNE\",\n",
        "    \"WNW\",\n",
        "    \"N\",\n",
        "    \"ENE\",\n",
        "    \"NE\",\n",
        "    \"E\",\n",
        "    \"SW\",\n",
        "    \"W\",\n",
        "    \"WSW\",\n",
        "    \"NNW\",\n",
        "    \"ESE\",\n",
        "    \"SSE\",\n",
        "    \"NW\",\n",
        "]\n",
        "print(len(orien))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfakrJ3km-zJ"
      },
      "source": [
        "Agrupo los valores de la variables categoricas de Direccion, que tiene los siguientes valores:\n",
        "\n",
        "['SSW', 'S', 'SE', 'NNE', 'WNW', 'N', 'ENE', 'NE', 'E', 'SW', 'W', 'WSW', 'NNW', 'ESE', 'SSE', 'NW']\n",
        "\n",
        "El criterio que empleo es asignar a cada punto cardenal el predominante, por ejemplo 'NNW' lo asigno a 'N'\n",
        "\n",
        "Para los valores como, por ejemplo 'NE' o 'SW' los asigno al ultimo punto cardinal de la notacion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvSAwJPqm-zK"
      },
      "outputs": [],
      "source": [
        "def agrupar_direcciones(direccion):\n",
        "    grupos_principales = {\n",
        "        \"N\": [\"N\", \"NNW\", \"NNE\"],\n",
        "        \"S\": [\"S\", \"SSW\", \"SSE\"],\n",
        "        \"E\": [\"E\", \"ENE\", \"ESE\", \"SE\", \"NE\"],\n",
        "        \"W\": [\"W\", \"WNW\", \"WSW\", \"SW\", \"NW\"],\n",
        "    }\n",
        "\n",
        "    for grupo, direcciones in grupos_principales.items():\n",
        "        if direccion in direcciones:\n",
        "            return grupo\n",
        "\n",
        "    return \"Otro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxQPcLJ8m-zK",
        "outputId": "acf019e5-f7a9-466d-d9cb-715700230c5f"
      },
      "outputs": [],
      "source": [
        "df_train[\"WindGustDir_Agrupado\"] = df_train[\"WindGustDir\"].apply(agrupar_direcciones)\n",
        "df_train[\"WindDir9am_Agrupado\"] = df_train[\"WindDir9am\"].apply(agrupar_direcciones)\n",
        "df_train[\"WindDir3pm_Agrupado\"] = df_train[\"WindDir3pm\"].apply(agrupar_direcciones)\n",
        "\n",
        "df_test[\"WindGustDir_Agrupado\"] = df_test[\"WindGustDir\"].apply(agrupar_direcciones)\n",
        "df_test[\"WindDir9am_Agrupado\"] = df_test[\"WindDir9am\"].apply(agrupar_direcciones)\n",
        "df_test[\"WindDir3pm_Agrupado\"] = df_test[\"WindDir3pm\"].apply(agrupar_direcciones)\n",
        "\n",
        "\n",
        "df_train = df_train.drop(\"WindGustDir\", axis=1)\n",
        "df_train = df_train.drop(\"WindDir9am\", axis=1)\n",
        "df_train = df_train.drop(\"WindDir3pm\", axis=1)\n",
        "\n",
        "df_test = df_test.drop(\"WindGustDir\", axis=1)\n",
        "df_test = df_test.drop(\"WindDir9am\", axis=1)\n",
        "df_test = df_test.drop(\"WindDir3pm\", axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdzWxBZVm-zK"
      },
      "source": [
        "##### Dummies WindGustDir\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCffP-mAm-zK"
      },
      "outputs": [],
      "source": [
        "d_WindGustDir_train = pd.get_dummies(\n",
        "    df_train[\"WindGustDir_Agrupado\"], dtype=int, drop_first=True\n",
        ")\n",
        "\n",
        "d_WindGustDir_train = d_WindGustDir_train.rename(\n",
        "    columns={\"N\": \"WindGustDir_N\", \"S\": \"WindGustDir_S\", \"W\": \"WindGustDir_W\"}\n",
        ")\n",
        "df_train = df_train.drop(\"WindGustDir_Agrupado\", axis=1)\n",
        "df_train = pd.concat([df_train, d_WindGustDir_train], axis=1)\n",
        "\n",
        "\n",
        "d_WindGustDir_test = pd.get_dummies(\n",
        "    df_test[\"WindGustDir_Agrupado\"], dtype=int, drop_first=True\n",
        ")\n",
        "\n",
        "d_WindGustDir_test = d_WindGustDir_test.rename(\n",
        "    columns={\"N\": \"WindGustDir_N\", \"S\": \"WindGustDir_S\", \"W\": \"WindGustDir_W\"}\n",
        ")\n",
        "df_test = df_test.drop(\"WindGustDir_Agrupado\", axis=1)\n",
        "df_test = pd.concat([df_test, d_WindGustDir_test], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjWOy4bVm-zK"
      },
      "source": [
        "##### Dummies WindDir9am\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgrDs1GOm-zK"
      },
      "outputs": [],
      "source": [
        "d_WindDir9am_train = pd.get_dummies(\n",
        "    df_train[\"WindDir9am_Agrupado\"], dtype=int, drop_first=True\n",
        ")\n",
        "d_WindDir9am_train = d_WindDir9am_train.rename(\n",
        "    columns={\"N\": \"WindDir9am_N\", \"S\": \"WindDir9am_S\", \"W\": \"WindDir9am_W\"}\n",
        ")\n",
        "df_train = df_train.drop(\"WindDir9am_Agrupado\", axis=1)\n",
        "df_train = pd.concat([df_train, d_WindDir9am_train], axis=1)\n",
        "\n",
        "\n",
        "d_WindDir9am_test = pd.get_dummies(\n",
        "    df_test[\"WindDir9am_Agrupado\"], dtype=int, drop_first=True\n",
        ")\n",
        "d_WindDir9am_test = d_WindDir9am_test.rename(\n",
        "    columns={\"N\": \"WindDir9am_N\", \"S\": \"WindDir9am_S\", \"W\": \"WindDir9am_W\"}\n",
        ")\n",
        "df_test = df_test.drop(\"WindDir9am_Agrupado\", axis=1)\n",
        "df_test = pd.concat([df_test, d_WindDir9am_test], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLeZMFx4m-zK"
      },
      "source": [
        "##### Dummies WindDir3pm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYpLtRDWm-zK"
      },
      "outputs": [],
      "source": [
        "d_WindDir3pm_train = pd.get_dummies(\n",
        "    df_train[\"WindDir3pm_Agrupado\"], dtype=int, drop_first=True\n",
        ")\n",
        "d_WindDir3pm_train = d_WindDir3pm_train.rename(\n",
        "    columns={\"N\": \"WindDir3pm_N\", \"S\": \"WindDir3pm_S\", \"W\": \"WindDir3pm_W\"}\n",
        ")\n",
        "df_train = df_train.drop(\"WindDir3pm_Agrupado\", axis=1)\n",
        "df_train = pd.concat([df_train, d_WindDir3pm_train], axis=1)\n",
        "\n",
        "\n",
        "d_WindDir3pm_test = pd.get_dummies(\n",
        "    df_test[\"WindDir3pm_Agrupado\"], dtype=int, drop_first=True\n",
        ")\n",
        "d_WindDir3pm_test = d_WindDir3pm_test.rename(\n",
        "    columns={\"N\": \"WindDir3pm_N\", \"S\": \"WindDir3pm_S\", \"W\": \"WindDir3pm_W\"}\n",
        ")\n",
        "df_test = df_test.drop(\"WindDir3pm_Agrupado\", axis=1)\n",
        "df_test = pd.concat([df_test, d_WindDir3pm_test], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBxCQKV6m-zK"
      },
      "outputs": [],
      "source": [
        "df_train.drop(\"Bimestre\", axis=1, inplace=True)\n",
        "df_train.drop(\"Date\", axis=1, inplace=True)\n",
        "\n",
        "df_test.drop(\"Bimestre\", axis=1, inplace=True)\n",
        "df_test.drop(\"Date\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J1aLbavm-zK",
        "outputId": "6ffbb4bd-f41d-45e1-c7b5-658a95655136"
      },
      "outputs": [],
      "source": [
        "columnas = df_train.columns\n",
        "print(columnas)\n",
        "print(len(columnas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RGK7XbO7m-zL",
        "outputId": "d77c2548-6742-419f-a08b-3ce180e11c53"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "sns.heatmap(\n",
        "    df_train.corr(),\n",
        "    annot=True,\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV9e_J7Zm-zL",
        "outputId": "af3a4263-246d-478d-fab0-73510abc2725"
      },
      "outputs": [],
      "source": [
        "df_train.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vMWGkWSm-zL"
      },
      "source": [
        "# Estandarizacion de datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LozRlhcCm-zL"
      },
      "outputs": [],
      "source": [
        "# df_train\n",
        "scaler = StandardScaler()\n",
        "df_train_estandarizado = scaler.fit_transform(df_train)\n",
        "df_train_estandarizado = pd.DataFrame(df_train_estandarizado, columns=df_train.columns)\n",
        "\n",
        "# df_test\n",
        "scaler = StandardScaler()\n",
        "df_test_estandarizado = scaler.fit_transform(df_test)\n",
        "df_test_estandarizado = pd.DataFrame(df_test_estandarizado, columns=df_test.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1pJFiGAm-zL"
      },
      "source": [
        "# Regularizacion de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWzjHi1mm-zL"
      },
      "outputs": [],
      "source": [
        "# X, y TRAIN\n",
        "X_train = df_train_estandarizado[\n",
        "    [\n",
        "        \"Rainfall\",\n",
        "        \"Evaporation\",\n",
        "        \"Sunshine\",\n",
        "        \"WindGustSpeed\",\n",
        "        \"RainToday\",\n",
        "        \"WindSpeed_Difference\",\n",
        "        \"Humidity_Difference\",\n",
        "        \"Cloud_Difference\",\n",
        "        \"Pressure_Difference\",\n",
        "        \"Temp_Difference\",\n",
        "        \"Dif_Temp_Max_Min\",\n",
        "        \"WindGustDir_N\",\n",
        "        \"WindGustDir_S\",\n",
        "        \"WindGustDir_W\",\n",
        "        \"WindDir9am_N\",\n",
        "        \"WindDir9am_S\",\n",
        "        \"WindDir9am_W\",\n",
        "        \"WindDir3pm_N\",\n",
        "        \"WindDir3pm_S\",\n",
        "        \"WindDir3pm_W\",\n",
        "    ]\n",
        "]\n",
        "y_train = df_train_estandarizado[\"RainfallTomorrow\"]\n",
        "\n",
        "# X, y TEST\n",
        "\n",
        "X_test = df_test_estandarizado[\n",
        "    [\n",
        "        \"Rainfall\",\n",
        "        \"Evaporation\",\n",
        "        \"Sunshine\",\n",
        "        \"WindGustSpeed\",\n",
        "        \"RainToday\",\n",
        "        \"WindSpeed_Difference\",\n",
        "        \"Humidity_Difference\",\n",
        "        \"Cloud_Difference\",\n",
        "        \"Pressure_Difference\",\n",
        "        \"Temp_Difference\",\n",
        "        \"Dif_Temp_Max_Min\",\n",
        "        \"WindGustDir_N\",\n",
        "        \"WindGustDir_S\",\n",
        "        \"WindGustDir_W\",\n",
        "        \"WindDir9am_N\",\n",
        "        \"WindDir9am_S\",\n",
        "        \"WindDir9am_W\",\n",
        "        \"WindDir3pm_N\",\n",
        "        \"WindDir3pm_S\",\n",
        "        \"WindDir3pm_W\",\n",
        "    ]\n",
        "]\n",
        "y_test = df_test_estandarizado[\"RainfallTomorrow\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnKU8RD6m-zL"
      },
      "source": [
        "# Lasso\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "eio9VZL7m-zL",
        "outputId": "6de17815-4e55-47ff-afe7-6615696954d7"
      },
      "outputs": [],
      "source": [
        "lasso = Lasso(alpha=0.1)  # alpha controla la fuerza de la regularización L1 (Lasso)\n",
        "\n",
        "lasso.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDS3lpyQm-zL",
        "outputId": "04a64530-f8db-455e-f755-1a1faff673b6"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCoeficientes del modelo Lasso:\")\n",
        "print(lasso.coef_)\n",
        "print(\"Lasso Score df_train:\", lasso.score(X_test, y_test))\n",
        "print(\"Lasso Score df_test:\", lasso.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M16RaFSxm-zL"
      },
      "source": [
        "# Ridge\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "vXFf_Bl4m-zM",
        "outputId": "4dea49cb-c195-4f7d-bb0d-e98162fd5ab2"
      },
      "outputs": [],
      "source": [
        "ridge = Ridge(alpha=0.1)  # alpha controla la fuerza de la regularización L2 (Ridge)\n",
        "ridge.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQoPKKtQm-zM",
        "outputId": "70d0c3aa-3d95-4895-8d8c-6edd9a01111b"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCoeficientes del modelo Ridge:\")\n",
        "print(ridge.coef_)\n",
        "print(\"Ridge Score df_train:\", ridge.score(X_train, y_train))\n",
        "print(\"Ridge Score df_test:\", ridge.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqKtVi1wm-zM"
      },
      "source": [
        "# Elasticnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "7xF4ChtLm-zM",
        "outputId": "e72c2b7a-c7ef-461e-feb3-0938f20e271c"
      },
      "outputs": [],
      "source": [
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
        "elasticnet.fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkRvsljFm-zM",
        "outputId": "d1622484-a140-4d0a-c720-3db8647821d7"
      },
      "outputs": [],
      "source": [
        "print(\"\\nCoeficientes del modelo ElasticNet:\")\n",
        "print(elasticnet.coef_)\n",
        "print(\"Elasticnet Score df_train:\", elasticnet.score(X_train, y_train))\n",
        "print(\"Elasticnet Score df_test:\", elasticnet.score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQv427Grm-zM"
      },
      "source": [
        "# Regresion Lineal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrWjzG2Rm-zM"
      },
      "source": [
        "Para evitar una Fuga de Datos voy a eliminar de mi dataset las variables RainTomorrow y RainfallTomorrow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeRCJhvOm-zM",
        "outputId": "e77dccba-249b-4a22-a14c-531a58827113"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "coefficients = model.coef_\n",
        "intercept = model.intercept_\n",
        "print(\"Coefficients:\", coefficients)\n",
        "print(\"Intercept:\", intercept)\n",
        "\n",
        "# MSE: Error Cuadratico Medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# R^2\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "# MAE: Error Absoluto Medio\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# RMSE: Raíz del Error Cuadrático Medio\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"\\nR^2: {r2}\\n\")\n",
        "print(f\"MSE(Error Cuadratico Medio): {mse}\\n\")\n",
        "print(f\"MAE(Error Absoluto Medio): {mae}\\n\")\n",
        "print(f\"RMSE(Raíz del Error Cuadrático Medio): {rmse}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimizacion de Hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definir la cuadrícula de hiperparámetros a explorar\n",
        "param_grid = {\n",
        "    'fit_intercept': [False, True],\n",
        "    'copy_X': [True, False]\n",
        "}\n",
        "# Inicializar la búsqueda en cuadrícula\n",
        "grid_search = GridSearchCV(LinearRegression(), param_grid, cv=5)\n",
        "\n",
        "# Realizar la búsqueda en cuadrícula\n",
        "grid_search.fit(X_train, y_train)\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Mejores hiperparámetros:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model2 = LinearRegression(copy_X=True, fit_intercept=False)\n",
        "\n",
        "model2.fit(X_train, y_train)\n",
        "\n",
        "y_predopt = model2.predict(X_test)\n",
        "\n",
        "coefficientsopt = model2.coef_\n",
        "interceptopt = model2.intercept_\n",
        "print(\"Coefficients:\", coefficientsopt)\n",
        "print(\"Intercept:\", interceptopt)\n",
        "\n",
        "# MSE: Error Cuadratico Medio\n",
        "mseopt = mean_squared_error(y_test, y_predopt)\n",
        "\n",
        "# R^2\n",
        "r2opt= r2_score(y_test, y_predopt)\n",
        "\n",
        "# MAE: Error Absoluto Medio\n",
        "maeopt = mean_absolute_error(y_test, y_predopt)\n",
        "\n",
        "# RMSE: Raíz del Error Cuadrático Medio\n",
        "rmseopt = np.sqrt(mean_squared_error(y_test, y_predopt))\n",
        "\n",
        "print(f\"\\nR^2: {r2opt}\\n\")\n",
        "print(f\"MSE(Error Cuadratico Medio): {mseopt}\\n\")\n",
        "print(f\"MAE(Error Absoluto Medio): {maeopt}\\n\")\n",
        "print(f\"RMSE(Raíz del Error Cuadrático Medio): {rmseopt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con la optimizacion de hiperparametros no aprecio variaciones en las metricas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtYWThkem-zM"
      },
      "source": [
        "## Gradiente descendiente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx-Nhyakm-zM"
      },
      "source": [
        "Probe usar el metodo de clase pero no pude correrlo debido a que me presentaba un error y no supe como solucionarlo.\n",
        "\n",
        "Por otro lado, busque en internet como hacerlo usando la ScikitLearn y lo implemente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuDDu0Iom-zM",
        "outputId": "7bc14642-c68b-431e-8600-fe1a0f1c9290"
      },
      "outputs": [],
      "source": [
        "# Regresión lineal utilizando SGDRegressor\n",
        "model_sgd = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "\n",
        "model_sgd.fit(X_train, y_train)\n",
        "\n",
        "y_pred_sgd = model_sgd.predict(X_test)\n",
        "\n",
        "# Coeficientes\n",
        "coefficients_sgd = model_sgd.coef_\n",
        "intercept_sgd = model_sgd.intercept_\n",
        "print(\"Coefficients (SGD):\", coefficients_sgd)\n",
        "print(\"Intercept (SGD):\", intercept_sgd)\n",
        "\n",
        "# MSE: Error Cuadrático Medio\n",
        "mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
        "\n",
        "# R^2\n",
        "r2_sgd = r2_score(y_test, y_pred_sgd)\n",
        "\n",
        "# MAE: Error Absoluto Medio\n",
        "mae_sgd = mean_absolute_error(y_test, y_pred_sgd)\n",
        "\n",
        "# RMSE: Raíz del Error Cuadrático Medio\n",
        "rmse_sgd = np.sqrt(mse_sgd)\n",
        "\n",
        "print(f\"\\nR^2 (SGD): {r2_sgd}\\n\")\n",
        "print(f\"MSE (SGD): {mse_sgd}\\n\")\n",
        "print(f\"MAE (SGD): {mae_sgd}\\n\")\n",
        "print(f\"RMSE (SGD): {rmse_sgd}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL8e0dckwxgl"
      },
      "outputs": [],
      "source": [
        "y_train_gd = y_train.values.reshape(-1, 1)\n",
        "y_test_gd = y_test.values.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gMgrBygm-zN"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(X_train, y_train, X_test, y_test, lr=0.01, epochs=100):\n",
        "    \"\"\"\n",
        "    shapes:\n",
        "        X_train = nxm\n",
        "        y_train = nx1\n",
        "        X_test = pxm\n",
        "        y_test = px1\n",
        "        W = mx1\n",
        "    \"\"\"\n",
        "    n = X_train.shape[0]\n",
        "    m = X_train.shape[1]\n",
        "\n",
        "    o = X_test.shape[0]\n",
        "\n",
        "    # Poner columna de unos a las matrices X\n",
        "    X_train = np.hstack((np.ones((n, 1)), X_train))\n",
        "    X_test = np.hstack((np.ones((o, 1)), X_test))\n",
        "\n",
        "\n",
        "    # Inicializar pesos aleatorios\n",
        "    W = np.random.randn(m+1).reshape(m+1, 1)\n",
        "\n",
        "    train_errors = []  # Para almacenar el error de entrenamiento en cada época\n",
        "    test_errors = []   # Para almacenar el error de prueba en cada época\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # Calcular predicción y error de entrenamiento\n",
        "        prediction_train = np.matmul(X_train, W)\n",
        "        error_train = y_train - prediction_train\n",
        "        #print(error_train)\n",
        "        train_mse = np.mean(error_train ** 2)\n",
        "        train_errors.append(train_mse)\n",
        "\n",
        "        # Calcular predicción y error de prueba\n",
        "        prediction_test = np.matmul(X_test, W)\n",
        "        error_test = y_test - prediction_test\n",
        "        test_mse = np.mean(error_test ** 2)\n",
        "        test_errors.append(test_mse)\n",
        "\n",
        "        # Calcular el gradiente y actualizar pesos\n",
        "        grad_sum = np.sum(error_train * X_train, axis=0)\n",
        "        grad_mul = -2/n * grad_sum  # 1xm\n",
        "        gradient = np.transpose(grad_mul).reshape(-1, 1)  # mx1\n",
        "\n",
        "        W = W - (lr * gradient)\n",
        "\n",
        "    # Graficar errores de entrenamiento y prueba\n",
        "    # Definir una figura\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Plotear errores de entrenamiento\n",
        "    plt.plot(train_errors, label='Error de entrenamiento')\n",
        "    # Plotear errores de prueba\n",
        "    plt.plot(test_errors, label='Error de test')\n",
        "    # Poner labels en los ejes\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Error cuadrático medio')\n",
        "    # Activar la leyenda\n",
        "    plt.legend()\n",
        "    # Poner titulo\n",
        "    plt.title('Error de entrenamiento y prueba vs iteraciones (GD)')\n",
        "    # Terminar y mostrar gráfico\n",
        "    plt.show()\n",
        "\n",
        "    return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "SnbkVLF5m-zN",
        "outputId": "6cf9d71a-8942-4a1a-8390-e418acbde9c0"
      },
      "outputs": [],
      "source": [
        "gradient_descent(X_train, y_train_gd, X_test, y_test_gd, lr=0.01, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQrg1dIy0AIM"
      },
      "outputs": [],
      "source": [
        "def stochastic_gradient_descent(X_train, y_train, X_test, y_test, lr=0.01, epochs=100):\n",
        "\n",
        "    n = X_train.shape[0]\n",
        "    m = X_train.shape[1]\n",
        "\n",
        "    X_train = np.hstack((np.ones((n, 1)), X_train))\n",
        "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "\n",
        "    W = np.random.randn(m + 1).reshape(-1, 1)\n",
        "\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "        # Permutación aleatoria de los datos\n",
        "        permutation = np.random.permutation(n)\n",
        "        X_train = X_train[permutation]\n",
        "        y_train = y_train[permutation]\n",
        "\n",
        "        for j in range(n):\n",
        "            # Obtener una muestra aleatoria de un solo dato para hacer SGD\n",
        "            x_sample = X_train[j]\n",
        "            y_sample = y_train[j][0]\n",
        "\n",
        "            prediction = np.matmul(x_sample, W)\n",
        "            error = y_sample - prediction\n",
        "            train_mse = error ** 2\n",
        "            train_errors.append(train_mse)\n",
        "\n",
        "            gradient = -2 * error * x_sample.T.reshape(-1, 1)\n",
        "\n",
        "            W = W - (lr * gradient)\n",
        "\n",
        "            prediction_test = np.matmul(X_test, W)\n",
        "            error_test = y_test - prediction_test\n",
        "            test_mse = np.mean(error_test ** 2)\n",
        "            test_errors.append(test_mse)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_errors, label='Error de entrenamiento')\n",
        "    plt.plot(test_errors, label='Error de prueba')\n",
        "    plt.xlabel('Iteración')\n",
        "    plt.ylabel('Error cuadrático medio')\n",
        "    plt.legend()\n",
        "    plt.title('Error de entrenamiento y prueba vs iteraciones (SGD)')\n",
        "    plt.show()\n",
        "\n",
        "    return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "f91nJ7va0HDz",
        "outputId": "e52d0974-2a01-4efa-f184-d1c27e53ecb6"
      },
      "outputs": [],
      "source": [
        "stochastic_gradient_descent(X_train, y_train_gd, X_test, y_test_gd, lr=0.01, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQiLxUh60wFM"
      },
      "outputs": [],
      "source": [
        "def mini_batch_gradient_descent(X_train, y_train, X_test, y_test, lr=0.01, epochs=100, batch_size=11):\n",
        "    n = X_train.shape[0]\n",
        "    m = X_train.shape[1]\n",
        "\n",
        "    X_train = np.hstack((np.ones((n, 1)), X_train))\n",
        "    X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))\n",
        "\n",
        "    W = np.random.randn(m + 1).reshape(-1, 1)\n",
        "\n",
        "    train_errors = []\n",
        "    test_errors = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        # Permutación aleatoria de los datos\n",
        "        permutation = np.random.permutation(n)\n",
        "        X_train = X_train[permutation]\n",
        "        y_train = y_train[permutation]\n",
        "\n",
        "\n",
        "        for j in range(0, n, batch_size):\n",
        "            # Obtener un lote (mini-batch) de datos\n",
        "            x_batch = X_train[j:j+batch_size, :]\n",
        "            y_batch = y_train[j:j+batch_size].reshape(-1, 1)\n",
        "\n",
        "            prediction = np.matmul(x_batch, W)\n",
        "            error = y_batch - prediction\n",
        "            train_mse = np.mean(error ** 2)\n",
        "            train_errors.append(train_mse)\n",
        "\n",
        "            gradient = -2 * np.matmul(x_batch.T, error) / batch_size\n",
        "\n",
        "            W = W - (lr * gradient)\n",
        "\n",
        "            prediction_test = np.matmul(X_test, W)\n",
        "            error_test = y_test - prediction_test\n",
        "            test_mse = np.mean(error_test ** 2)\n",
        "            test_errors.append(test_mse)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(train_errors, label='Error de entrenamiento')\n",
        "    plt.plot(test_errors, label='Error de prueba')\n",
        "    plt.xlabel('Iteración')\n",
        "    plt.ylabel('Error cuadrático medio')\n",
        "    plt.legend()\n",
        "    plt.title('Error de entrenamiento y prueba vs iteraciones (Mini-Batch GD)')\n",
        "    plt.show()\n",
        "\n",
        "    return W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "MyWc8Cvk05mk",
        "outputId": "92517784-d2b6-4a22-c66f-c137e016e6e1"
      },
      "outputs": [],
      "source": [
        "mini_batch_gradient_descent(X_train, y_train_gd, X_test, y_test_gd, lr=0.1, epochs=100, batch_size=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regresion Logistica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# y_train y y_test para la variale RainTomorrow\n",
        "y_train_rl = df_train[\"RainTomorrow\"]\n",
        "y_test_rl = df_test[\"RainTomorrow\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjunto de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_model = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "logistic_model.fit(X_train, y_train_rl)\n",
        "y_pred_train_rl = logistic_model.predict(X_train)\n",
        "y_pred_test_rl = logistic_model.predict(X_test)\n",
        "balanced_accuracy = balanced_accuracy_score(y_train_rl, y_pred_train_rl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular métricas\n",
        "balanced_accuracy = balanced_accuracy_score(y_train_rl, y_pred_train_rl)\n",
        "# accuracy_logreg = accuracy_score(y_train_rl, y_pred_train_rl)\n",
        "confusion_matrix_logreg = confusion_matrix(y_train_rl, y_pred_train_rl)\n",
        "classification_report_logreg = classification_report(y_train_rl, y_pred_train_rl)\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"Métricas para logreg:\")\n",
        "print(f'Accuracy balanceado: {balanced_accuracy}')\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix_logreg)\n",
        "print(\"Reporte de clasificación:\")\n",
        "print(classification_report_logreg)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_logreg)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso de aplicacion, el interes se encuentra puesto en predecir si llovera o no, por lo que nos importa principalmente el caso donde se de un **verdadero positivo**, el modelo obtuvo sobre el **conjunto de entrenamiento** un total de 3559 casos en los que predijo que lloveria y llovio, por otra parte hay un total de 3569 casos de **falsos positivos**, predijo que lloveria y no llovio.\n",
        "\n",
        "Observando la metrica **F1 Score** para los casos **positivos** es de 0.60, por lo que el modelo no es muy eficiente a la hora de predecir esta clase, mientras que para los casos **negativos** obtiene una metrica de 0.83 indicando que tiene mas facilidad para predecir dicha clase, esto puede que tenga que ver con el desbalanceo de clases que hay en el conjunto de datos, hay una mayor presencia de clases negativas que positivas, lo que tiene sentido con la realidad, debido a que generalmente son mas los dias en los que no llueve que los que si."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conjunto de test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular métricas\n",
        "balanced_accuracy = balanced_accuracy_score(y_test_rl, y_pred_test_rl)\n",
        "# accuracy_logreg = accuracy_score(y_train_rl, y_pred_train_rl)\n",
        "confusion_matrix_logreg = confusion_matrix(y_test_rl, y_pred_test_rl)\n",
        "classification_report_logreg = classification_report(y_test_rl, y_pred_test_rl)\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"Métricas para logreg:\")\n",
        "print(f'Accuracy balanceado: {balanced_accuracy}')\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix_logreg)\n",
        "print(\"Reporte de clasificación:\")\n",
        "print(classification_report_logreg)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_logreg)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para el **conjunto de test** vemos tambien valores muy parejos entre los **verdaderos positivos** y **falsos positivos** y obtuvimos los mismos valores que en el conjunto de entrenamiento en la metrica **F1 Score** para ambas clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Curva ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtengo las probabilidades de predicción del modelo para los datos de entrenamiento y prueba\n",
        "y_probs_train = logistic_model.predict_proba(X_train)[:, 1]\n",
        "y_probs_test = logistic_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculo la ROC y el AUC para los datos de entrenamiento y prueba\n",
        "fpr_train, tpr_train, thresholds_train = roc_curve(y_train_rl, y_probs_train, pos_label= 1)\n",
        "roc_auc_train = auc(fpr_train, tpr_train)\n",
        "\n",
        "fpr_test, tpr_test, thresholds_test = roc_curve(y_test_rl, y_probs_test, pos_label= 1)\n",
        "roc_auc_test = auc(fpr_test, tpr_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grafico la curva ROC para los datos de entrenamiento y prueba\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_train, tpr_train, color='darkorange', lw=2, label='Curva ROC para entrenamiento (AUC = %0.2f)' % roc_auc_train)\n",
        "plt.plot(fpr_test, tpr_test, color='green', lw=2, label='Curva ROC para prueba (AUC = %0.2f)' % roc_auc_test)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"El área bajo la curva para entrenamiento es de: {roc_auc_train}\")\n",
        "print(f\"El área bajo la curva para prueba es de: {roc_auc_test}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcula la distancia euclidiana entre cada punto de la curva ROC y (0,1) para los datos de entrenamiento\n",
        "distances_train = np.sqrt((1 - tpr_train)**2 + fpr_train**2)\n",
        "\n",
        "# Encuentra el índice del punto que minimiza la distancia para los datos de entrenamiento\n",
        "min_index_train = np.argmin(distances_train)\n",
        "\n",
        "# Obtiene el umbral óptimo para los datos de entrenamiento\n",
        "optimal_threshold_train = thresholds_train[min_index_train]\n",
        "\n",
        "# Calcula la distancia euclidiana entre cada punto de la curva ROC y (0,1) para los datos de prueba\n",
        "distances_test = np.sqrt((1 - tpr_test)**2 + fpr_test**2)\n",
        "\n",
        "# Encuentra el índice del punto que minimiza la distancia para los datos de prueba\n",
        "min_index_test = np.argmin(distances_test)\n",
        "\n",
        "# Obtiene el umbral óptimo para los datos de prueba\n",
        "optimal_threshold_test = thresholds_test[min_index_test]\n",
        "\n",
        "print(\"Umbral óptimo para entrenamiento:\", optimal_threshold_train)\n",
        "print(\"Umbral óptimo para prueba:\", optimal_threshold_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizacion de hiperparametros para la **Regresion Logistica**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Definir la cuadrícula de hiperparámetros a explorar\n",
        "param_grid = {\n",
        "    'penalty': [None, 'l1', 'l2'],\n",
        "    'dual': [True, False],\n",
        "    'C': [1, 1.5, 2, 2.5, 3],\n",
        "    'fit_intercept': [True, False],\n",
        "    'class_weight': [None, 'balanced', {0: 0.5, 1: 0.5}]\n",
        "}\n",
        "\n",
        "# Definir la cuadrícula de hiperparámetros a explorar\n",
        "param_grid_2 = {\n",
        "    'penalty': ['l1', 'l2'],  # Penalidades válidas\n",
        "    'C': [0.1, 1, 10],  # Fuerza de regularización\n",
        "    'class_weight': [None, 'balanced'],  # Pesos de clase\n",
        "    'solver': ['liblinear', 'saga']  # Solver para optimización\n",
        "}\n",
        "\n",
        "\n",
        "# Inicializar la búsqueda en cuadrícula\n",
        "grid_search = GridSearchCV(logreg, param_grid_2, cv=5)\n",
        "\n",
        "# Realizar la búsqueda en cuadrícula\n",
        "grid_search.fit(X_train, y_train_lg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener los mejores hiperparámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Mejores hiperparámetros:\", best_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg_opt = LogisticRegression(C=0.1, class_weight=None, penalty='l2', solver='saga')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrenar un modelo de regresión logística\n",
        "logreg_opt.fit(X_train, y_train_lg)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred_opt = logreg_opt.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Calcular métricas\n",
        "accuracy_logreg = accuracy_score(y_test_lg, y_pred_opt)\n",
        "# accuracy_logreg_2D = accuracy_score(y_test_2D, y_pred_2D)\n",
        "\n",
        "confusion_matrix_logreg = confusion_matrix(y_test_lg, y_pred_opt)\n",
        "# confusion_matrix_logreg_2D = confusion_matrix(y_test_2D, y_pred_2D)\n",
        "\n",
        "classification_report_logreg = classification_report(y_test_lg, y_pred_opt)\n",
        "# classification_report_logreg_2D = classification_report(y_test_2D, y_pred_2D)\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"Métricas para logreg_opt:\")\n",
        "print(f'Precisión: {accuracy_logreg}')\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix_logreg)\n",
        "print(\"Reporte de clasificación:\")\n",
        "print(classification_report_logreg)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_logreg)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso al usar GridSearch para optimizar hiperparametros no se observan cambios en las metricas, con respecto al modelo base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn import metrics\n",
        "y_probs_opt = logreg_opt.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculo la ROC y el AUC\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test_lg, y_probs_opt)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Grafico la curva ROC\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Balanceo de clases con SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica SMOTE para balancear las clases\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train_lg)\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "print(\"Mejores parámetros:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg_opt.fit(X_resampled, y_resampled)\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred_opt_smote = logreg_opt.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Calcular métricas\n",
        "accuracy_logreg = accuracy_score(y_test_lg, y_pred_opt_smote)\n",
        "# accuracy_logreg_2D = accuracy_score(y_test_2D, y_pred_2D)\n",
        "\n",
        "confusion_matrix_logreg = confusion_matrix(y_test_lg, y_pred_opt_smote)\n",
        "# confusion_matrix_logreg_2D = confusion_matrix(y_test_2D, y_pred_2D)\n",
        "\n",
        "classification_report_logreg = classification_report(y_test_lg, y_pred_opt_smote)\n",
        "# classification_report_logreg_2D = classification_report(y_test_2D, y_pred_2D)\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"Métricas para logreg_opt:\")\n",
        "print(f'Precisión: {accuracy_logreg}')\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix_logreg)\n",
        "print(\"Reporte de clasificación:\")\n",
        "print(classification_report_logreg)\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_logreg)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "from sklearn import metrics\n",
        "y_pred_opt_smote = logreg_opt.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculo la ROC y el AUC\n",
        "fpr, tpr, thresholds = metrics.roc_curve(y_test_lg, y_pred_opt_smote)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Grafico la curva ROC\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oversampler = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = oversampler.fit_resample(X_train, y_train_lg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(y_resampled[y_resampled==1]), len(y_resampled[y_resampled==0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg.fit(X_resampled, y_resampled)\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print(\"Resultados regresión logística con Oversampling:\\n\")\n",
        "print(classification_report(y_test_lg, y_pred))\n",
        "print(confusion_matrix(y_test_lg, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtengo las probabilidades de predicción del modelo\n",
        "y_probs = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculo la ROC y el AUC\n",
        "fpr, tpr, thresholds = roc_curve(y_test_lg, y_probs)\n",
        "roc_auc = auc(fpr, tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grafico la curva ROC\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos (FPR)')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos (TPR)')\n",
        "plt.title('Curva ROC')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejercicio 6 - Modelos Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo Base de Regresion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_rl = X_train[[\"Rainfall\", \"RainToday\"]]\n",
        "X_test_rl = X_test[[\"Rainfall\", \"RainToday\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_rl = LinearRegression()\n",
        "\n",
        "model_rl.fit(X_train_rl, y_train)\n",
        "\n",
        "y_pred = model_rl.predict(X_test_rl)\n",
        "\n",
        "coefficients = model_rl.coef_\n",
        "intercept = model_rl.intercept_\n",
        "print(\"Coefficients:\", coefficients)\n",
        "print(\"Intercept:\", intercept)\n",
        "\n",
        "# MSE: Error Cuadratico Medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "# R^2\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "# MAE: Error Absoluto Medio\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "# RMSE: Raíz del Error Cuadrático Medio\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"\\nR^2: {r2}\\n\")\n",
        "print(f\"MSE(Error Cuadratico Medio): {mse}\\n\")\n",
        "print(f\"MAE(Error Absoluto Medio): {mae}\\n\")\n",
        "print(f\"RMSE(Raíz del Error Cuadrático Medio): {rmse}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El coeficiente **R²** es de 0.084 lo que indica que el modelo no tiene una gran capacidad para explicar la variabilidad de los datos. El modelo no captura adecuadamente las relaciones entre las variables predictoras y la variable objetivo.\n",
        "\n",
        "El **MSE** con un valor de 0.9116 nos demuestra que las predicciones del modelo estan bastate dispersas respecto a los valores observados, se puede decir que su ajuste es bastante malo.\n",
        "\n",
        "**Conclusion**: Las metricas obtenidas nos sugieren que las predicciones del modelo no son precisas y tienen un error significativo. Esto podría ser porque el modelo es demasiado simple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modelo Base de Clasificacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dummy_random = DummyClassifier(strategy='uniform')\n",
        "dummy_random.fit(X_train_rl, y_train_rl)\n",
        "y_pred_random = dummy_random.predict(X_test_rl)\n",
        "\n",
        "accuracy_random = accuracy_score(y_test_rl, y_pred_random)\n",
        "print(\"Accuracy del clasificador aleatorio:\", accuracy_random)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ejercicio 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validacion Cruzada K-Folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, LeaveOneOut\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "clf = RandomForestClassifier(n_estimators=50, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_k, x_val_k, y_train_k, y_val_k = train_test_split(X_train_rl, y_train_rl, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_val_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_train_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_val_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_strategies = [\n",
        "    (\"KFold\", KFold(n_splits=5, shuffle=True, random_state=42))\n",
        "]\n",
        "\n",
        "results = {}\n",
        "for name, cv in cv_strategies:\n",
        "    scores = cross_val_score(clf, x_val_k, y_val_k, cv=cv)\n",
        "    results[name] = scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "for name, scores in results.items():\n",
        "    plt.plot(range(1, len(scores) + 1), scores, marker='o', label=name)\n",
        "\n",
        "plt.xlabel(\"Fold\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Estrategias de validación cruzada\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results['KFold']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for strategy in cv_strategies:\n",
        "  print('Media para la estrategia', strategy[0],':',results['KFold'].mean())\n",
        "  print('Desvío estándar para la estrategia', strategy[0],':',results['KFold'].std())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explicabilidad Local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = X_train.columns.values\n",
        "explainer = shap.LinearExplainer(logistic_model, X_train, feature_names=feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap_values = explainer.shap_values(X_test)\n",
        "explainer.expected_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "explanation = shap.Explanation(values=shap_values[0], base_values=explainer.expected_value, feature_names=feature_names)\n",
        "shap.plots.bar(explanation, max_display=21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A nivel local, **WindSpeed_Diference** y **Cloud_Difference** son las variables que mas impactan sobre la prediccion de **RainTomorrow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explicabilidad global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "explanation = shap.Explanation(values=shap_values, base_values=explainer.expected_value, feature_names=feature_names, data=X_test_rl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.plots.bar(explanation, max_display=21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las variables de mayor importancia a nivel global son **Sunshine** y **WindSpeed_Difference**, esta ultima tambien observada en la explicabilidad local."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Redes Neuronales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetworkTensorFlow:\n",
        "    \"\"\"\n",
        "        Este es un modelo simple con TensorFlow para resolver el mismo problema. \n",
        "        En esta clase, (1) se construye el modelo.\n",
        "        (2) Se define como se fitea el modelo\n",
        "        (3) Y como se hacen las predicciones.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "            Construye el modelo\n",
        "            Para construir el modelo es necesario una arquitectura, un optimizador y una función de pérdida.\n",
        "            La arquitectura se construye con el método Sequential, que basicamente lo que hace es colocar \n",
        "            secuencialmente las capas que uno desea.\n",
        "            Las capas \"Dense\" son las fully connected dadas en clase.\n",
        "            Se agrega una capa oculta que recibe un input de tamaño 2,\n",
        "            y una capa de salida de regresión (una única neurona)\n",
        "            En todos los casos se define una sigmoidea como función de activación (prueben otras!)\n",
        "\n",
        "            El optimizador y la función de pérdida se especifican dentro de un compilador.\n",
        "\n",
        "            Con este método, lo que se devuelve es el modelo sin entrenar, sería equivalente a escribir LinearRegression() \n",
        "            en el caso de la regresión lineal.\n",
        "        \"\"\"\n",
        "\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(2, activation='sigmoid', input_shape=(20,)),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        ### imprimimos la cantidad de parámetros a modo de ejemplo\n",
        "        print(\"n° de parámetros:\", model.count_params())\n",
        "        return model\n",
        "    \n",
        "    def fit(self, X, y, lr=0.1, epochs=20000):\n",
        "        ### esta es la función donde se entrena el modelo, fijarse que hay un learning rate e iteraciones.\n",
        "        ### la función que fitea devuelve una historia de pérdida, que vamos a guardar para graficar la evolución.\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        history = self.model.fit(X, y, epochs=epochs, verbose=0)\n",
        "        return history.history['loss']\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        predictions = self.model.predict(X)\n",
        "        return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regresion Lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'MSE: {mse}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn_tensorflow = NeuralNetworkTensorFlow()\n",
        "loss_history = nn_tensorflow.fit(X_train, y_train, lr=0.1, epochs=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(loss_history, label='NN')\n",
        "plt.axhline(mse, color='red', label='linear',linestyle = '-')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimizacion de hiperparametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
        "    model = Sequential()\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        num_units = trial.suggest_int(f'n_units_layer_{i}', 4, 128) # la cantidad de neuronas de cada capa tambien se puede pasar como hiperparámetro\n",
        "        # activations = trial.suggest_categorical(f'')\n",
        "        model.add(Dense(num_units, activation='sigmoid')) # capas densas con activacion ReLU\n",
        "\n",
        "    # capa de salida\n",
        "    model.add(Dense(1)) # 3 son las clases de salida\n",
        "\n",
        "    # compilar\n",
        "    model.compile(optimizer='Adagrad', loss='categorical_crossentropy', metrics=['mse'])\n",
        "\n",
        "    epochs = trial.suggest_int('epochs', 5, 100)\n",
        "\n",
        "    # entrenar\n",
        "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "    # evaluar\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "    return score[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# crear un estudio de Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# obtener los mejores hiperparámetros\n",
        "best_params = study.best_params\n",
        "print(\"Best parámetros encontrados:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al utilizar optuna, obtuve diferentes resultados, voy a quedarme con el siguiente.\n",
        "Mejores parámetros encontrados: {'num_layers': 3, 'n_units_layer_0': 54, 'n_units_layer_1': 95, 'n_units_layer_2': 92, 'epochs': 29}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nueva clase de nn_tensorflow con hiperparametros ajustados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "class NeuralNetworkTensorFlowOpt:\n",
        "    def __init__(self):\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        num_layers = 3\n",
        "        n_units_layer_0 = 54\n",
        "        n_units_layer_1 = 95\n",
        "\n",
        "        model = tf.keras.Sequential()\n",
        "        model.add(tf.keras.layers.Dense(n_units_layer_0, activation='sigmoid', input_shape=(20,)))\n",
        "\n",
        "        for _ in range(1, num_layers):\n",
        "            model.add(tf.keras.layers.Dense(n_units_layer_1, activation='sigmoid'))\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "        \n",
        "        ### imprimimos la cantidad de parámetros a modo de ejemplo\n",
        "        print(\"n° de parámetros:\", model.count_params())\n",
        "        return model\n",
        "    \n",
        "    def fit(self, X, y, lr=0.1, epochs=29):\n",
        "        ### esta es la función donde se entrena el modelo, fijarse que hay un learning rate e iteraciones.\n",
        "        ### la función que fitea devuelve una historia de pérdida, que vamos a guardar para graficar la evolución.\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        history = self.model.fit(X, y, epochs=epochs, verbose=0)\n",
        "        return history.history['loss']\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.array(X)\n",
        "        predictions = self.model.predict(X)\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nn_tensorflowOpt = NeuralNetworkTensorFlowOpt()\n",
        "loss_history_opt = nn_tensorflowOpt.fit(X_train, y_train, lr=0.1, epochs=29)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(loss_history_opt, label='NN')\n",
        "plt.axhline(mse, color='red', label='linear',linestyle = '-')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Puedo apreciar que en esta ocacion se logra obtener un valor de MSE menor al obtenido en el modelo de Regresion Lineal implementado anteriormente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regresion Logistica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f'Precisión obtenida en el modelo implemtado anteriormente: {accuracy_logreg}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizo el y_train_rl generado en la seccion de Regresion Logistica ya que tiene valores 1 y 0 correspondientes a cada clase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defino una nueva funcion objetivo adaptandola a un modelo de clasificacion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "\n",
        "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
        "    model = Sequential()\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        num_units = trial.suggest_int(f'n_units_layer_{i}', 4, 128) # la cantidad de neuronas de cada capa tambien se puede pasar como hiperparámetro\n",
        "        # activations = trial.suggest_categorical(f'')\n",
        "        model.add(Dense(num_units, activation='relu')) # capas densas con activacion ReLU\n",
        "\n",
        "    # capa de salida\n",
        "    model.add(Dense(1, activation='sigmoid')) # 2 son las clases de salida\n",
        "\n",
        "    # compilar\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Sugerir el número de epochs como hiperparámetro\n",
        "    epochs = trial.suggest_int('epochs', 5, 50)\n",
        "\n",
        "    # entrenar\n",
        "    model.fit(X_train, y_train_lg, validation_data=(X_test, y_test_lg), epochs=5, batch_size=32, verbose=0)\n",
        "\n",
        "    # evaluar\n",
        "    score = model.evaluate(X_test, y_test_lg, verbose=0)\n",
        "    return score[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# crear un estudio de Optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "# obtener los mejores hiperparámetros\n",
        "best_params = study.best_params\n",
        "print(\"Best parámetros encontrados:\", best_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mejores parámetros encontrados: {'num_layers': 3, 'n_units_layer_0': 41, 'n_units_layer_1': 24, 'n_units_layer_2': 83, 'epochs': 39}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modelo de red neuronal\n",
        "model = Sequential()\n",
        "model.add(Dense(90, activation='relu'))  # Capa oculta con 38 neuronas y función de activación ReLU\n",
        "model.add(Dense(90, activation='relu'))  # Capa oculta con 38 neuronas y función de activación ReLU\n",
        "# model.add(Dense(38, activation='relu'))  # Capa oculta con 38 neuronas y función de activación ReLU\n",
        "model.add(Dense(1, activation='sigmoid'))  # Capa de salida con 1 neurona  y función de activación sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# modelo de red neuronal\n",
        "model = Sequential()\n",
        "model.add(Dense(38, activation='relu'))  # Capa oculta con 38 neuronas y función de activación ReLU\n",
        "model.add(Dense(38, activation='relu'))  # Capa oculta con 38 neuronas y función de activación ReLU\n",
        "model.add(Dense(38, activation='relu'))  # Capa oculta con 38 neuronas y función de activación ReLU\n",
        "model.add(Dense(1, activation='sigmoid'))  # Capa de salida con 1 neurona  y función de activación sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train_rl, epochs=48, batch_size=32, validation_data=(X_test, y_test_rl))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hacer predicciones\n",
        "y_pred2_rn = model.predict(X_test)\n",
        "y_pred = (y_pred2_rn > 0.5).astype(int)  # Clasificación binaria\n",
        "\n",
        "# Métricas para el modelo sin optimización\n",
        "f1_score_rn = f1_score(y_test_rl, y_pred, average='weighted')\n",
        "\n",
        "print(f\"F1 Score del modelo: {f1_score_rn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular métricas\n",
        "balanced_accuracy = balanced_accuracy_score(y_test_rl, y_pred)\n",
        "# accuracy_logreg = accuracy_score(y_train_rl, y_pred_train_rl)\n",
        "confusion_matrix_logreg = confusion_matrix(y_test_rl, y_pred)\n",
        "classification_report_logreg = classification_report(y_test_rl, y_pred)\n",
        "\n",
        "# Imprimir métricas\n",
        "print(\"Métricas para logreg:\")\n",
        "print(f'Accuracy balanceado: {balanced_accuracy}')\n",
        "print(\"Matriz de confusión:\")\n",
        "print(confusion_matrix_logreg)\n",
        "print(\"Reporte de clasificación:\")\n",
        "print(classification_report_logreg)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_logreg)\n",
        "disp.plot()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usando redes neuronales para clasificar, obtuve valores muy similares a los obtenidos en el modelo implementado anteriormente, los valores de **F1 Score** para cada clase se encuentran casi invariantes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explicabilidad Local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "background = X_train.sample(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names = X_train.columns.values\n",
        "explainer = shap.KernelExplainer(model, background)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap_values = explainer.shap_values(instance_to_explain)\n",
        "explainer.expected_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "explanation = shap.Explanation(values=shap_values[0], base_values=explainer.expected_value, feature_names=feature_names)\n",
        "shap.plots.bar(explanation, max_display=21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A nivel local, **WindSpeed_Diference** y **Cloud_Difference** son las variables que mas impactan sobre la prediccion de **RainTomorrow**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explicabilidad global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "explanation = shap.Explanation(values=shap_values, base_values=explainer.expected_value, feature_names=feature_names, data=X_test_rl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "shap.plots.bar(explanation, max_display=21)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las variables de mayor importancia a nivel global son **Sunshine** y **WindSpeed_Difference**, esta ultima tambien observada en la explicabilidad local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un conjunto de muestras de fondo\n",
        "background = X_train.sample(100)  \n",
        "# Crear el objeto explainer SHAP utilizando KernelExplainer\n",
        "explainer = shap.KernelExplainer(model, background)\n",
        "instance_to_explain = X_test.iloc[0:1].values  # Seleccionar la línea 0\n",
        "# Calcular los valores SHAP para los datos de prueba\n",
        "shap_values = explainer.shap_values(instance_to_explain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener el valor esperado (base value)\n",
        "expected_value = explainer.expected_value\n",
        "# Hacer predicciones\n",
        "index = 0  # índice de la instancia que queremos explicar\n",
        "predicted_proba = model.predict(X_test)[index]  # Predicción de probabilidad\n",
        "predicted_class = np.argmax(predicted_proba)  # Clase predicha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear la explicación para la instancia específica\n",
        "explanation = shap.Explanation(values=shap_values[0], base_values=expected_value, feature_names=feature_names)\n",
        "\n",
        "# Visualizar la explicación\n",
        "shap.initjs()\n",
        "shap.plots.bar(explanation, max_display=len(feature_names))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
